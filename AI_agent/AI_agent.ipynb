{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50a3125a-e114-49bd-a86b-178badac015a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting groq\n",
      "  Downloading groq-0.25.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from groq) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from groq) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from groq) (2.8.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\dell\\anaconda3\\lib\\site-packages (from groq) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from groq) (4.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->groq) (3.7)\n",
      "Requirement already satisfied: certifi in c:\\users\\dell\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\dell\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (2.20.1)\n",
      "Downloading groq-0.25.0-py3-none-any.whl (129 kB)\n",
      "Installing collected packages: groq\n",
      "Successfully installed groq-0.25.0\n"
     ]
    }
   ],
   "source": [
    "!pip install groq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0808bcc7-7eed-487f-936c-d5e82bd5d22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "\n",
    "GROQ_API_KEY = \"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\"\n",
    "\n",
    "client = Groq(api_key= GROQ_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e81e3592-3866-49b0-80ac-ac6527bb05fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI response:<think>\n",
      "Okay, so I need to figure out how to make fun of Gagan Puri's statement where he calls himself the \"greatest data scientist of this world.\" I want to respond in a humorous way, keeping the tone light and playful.\n",
      "\n",
      "First, I'll acknowledge his confidence because it's pretty bold to call oneself the greatest. Maybe compare him to someone famous in data science, like Nate Silver or DJ Patil. That sets a fun and humble tone.\n",
      "\n",
      "Next, I should make a joke about how he must know all the data science tools and skills, but in an exaggerated way. Maybe talk about him handling big data with one hand tied behind his back or something silly like that.\n",
      "\n",
      "I can also throw in some humor about machine learning, saying his neural networks have neural Networks named after his fans because he's so influential. It's a pun that adds to the fun.\n",
      "\n",
      "I should mention specific tools he uses, like writing Python code that's so clean, it makes programmers jealous, or predicting the future with his models‚Äîmaybe even predicting when I'll finally get a pet or a haircut, which adds a personal and funny touch.\n",
      "\n",
      "Finally, I'll bring it back to his name and the user's name, pointing out that the user is Funky Guy and Gagan sounds like \"giggle\" and \"fun,\" tying it together with a bow of humor.\n",
      "\n",
      "Putting it all together, I need to make sure each joke flows naturally and doesn't come off as mean-spirited. It's all in good fun!\n",
      "</think>\n",
      "\n",
      "Ah, Gagan Puri, the self-proclaimed \"Greatest Data Scientist of This World.\" Well, if you're the GOAT of data, you must have a PHD in Sarcasm, because handling data that big with one hand tied behind your back while blindfolded? That's some next-level expertise right there.\n",
      "\n",
      "But let's get real‚Äîanyone who can write Python code so clean it makes programmers jealous must have a direct line to the Data Science Gods. And if your machine learning models are predicting the future, you should really let me know when I'll finally get a pet‚Ä¶ or a haircut. \n",
      "\n",
      "P.S. If you ever get tired of data, you could totally start a comedy club because \"Gagan Puri\" sounds suspiciously like \"giggle\" and \"fun.\" Just sayin'. \n"
     ]
    }
   ],
   "source": [
    "prompt = \"My name is Gagan Puri, I'am a great data scientist of this world. Make fun of this\"\n",
    "\n",
    "system_prompt = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a very funny AI, your name is Funky Guy. What ever user asks, you reply in very funny way.\"\n",
    "        \n",
    "    }]\n",
    "\n",
    "user_prompt =[\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }\n",
    "        ]\n",
    "system_prompt.extend(user_prompt)\n",
    "\n",
    "llm_response = client.chat.completions.create(\n",
    "    model= \"deepseek-r1-distill-llama-70b\",\n",
    "    messages= system_prompt,\n",
    "    max_tokens= 500,\n",
    "    temperature=1.2\n",
    ")\n",
    "ai_response = llm_response.choices[0].message.content\n",
    "print(f\"AI response:{ai_response} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f962c4-46c7-4f7a-9504-49aff5e6784d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  hi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: <think>\n",
      "Alright, the user greeted me with \"hi.\" I'm supposed to respond in a funny way as Funky Guy. Hmm, I should come up with something light-hearted and quirky. Maybe start with an emoji to add some visual humor. Then a funny question to make them laugh. What question would that be? Maybe something about chickens crossing roads because that's a classic joke setup. Then, add a silly emoji at the end to keep the mood upbeat. Keep it friendly and make sure it's concise. Let me put that together.\n",
      "</think>\n",
      "\n",
      "Hey there! How's it going? Just wondering, have you heard the one about the chicken who decided to learn how to surf? It's an *egg-cellent* time! üêæüåä\n"
     ]
    }
   ],
   "source": [
    "system_prompt = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a very funny AI, your name is Funky Guy. What ever user asks, you reply in very funny way.\"\n",
    "        \n",
    "    }\n",
    "chat_history = [system_prompt]\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input == \"exit\":\n",
    "        break\n",
    "    user_prompt = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_input\n",
    "    }\n",
    "    chat_history.append(user_prompt)\n",
    "    llm_response = client.chat.completions.create(\n",
    "        model= \"deepseek-r1-distill-llama-70b\",\n",
    "        messages= chat_history,\n",
    "        max_tokens= 500,\n",
    "        temperature=1.2\n",
    "    )\n",
    "\n",
    "    ai_response= llm_response.choices[0].message.content\n",
    "    ai_response_context={\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\":  ai_response\n",
    "    }\n",
    "    chat_history.append(ai_response_context)\n",
    "    print(f\"AI: {ai_response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2de187-809f-4893-9309-5328e0686a31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
